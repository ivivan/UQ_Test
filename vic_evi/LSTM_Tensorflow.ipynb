{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of nmt_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b605156-d7d2-4714-9b2d-2663f0a5d197"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = '/content/drive/My Drive/DLDATA/Crop/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEhZhSeCw21"
      },
      "source": [
        "class AttentionModel(tf.keras.Model):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, recurrent_layers, dropout_p):\n",
        "        super(AttentionModel, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.recurrent_layers = recurrent_layers\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.input_embeded = tf.keras.layers.Dense(\n",
        "            hidden_dim//2, activation='tanh')\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_p)\n",
        "\n",
        "        self.rnn_layers = []\n",
        "        for _ in range(0, recurrent_layers):\n",
        "\n",
        "            rnn_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim,\n",
        "                                                                           return_sequences=True,\n",
        "                                                                           return_state=True))\n",
        "            self.rnn_layers.append(rnn_layer)\n",
        "\n",
        "        self.self_attention = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(hidden_dim*2, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        self.scale = 1.0/np.sqrt(hidden_dim)\n",
        "\n",
        "        self.output_linear = tf.keras.layers.Dense(self.hidden_dim)\n",
        "        self.label = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, input_sentences):\n",
        "\n",
        "        input = self.input_embeded(input_sentences)\n",
        "        input = self.dropout(input)\n",
        "\n",
        "        for i, _ in enumerate(self.rnn_layers):\n",
        "            output, forward_h, forward_c, backward_h, backward_c = self.rnn_layers[i](\n",
        "                input)\n",
        "\n",
        "        attn_ene = self.self_attention(output)\n",
        "\n",
        "        # scale\n",
        "        attn_ene = tf.math.scalar_mul(self.scale, attn_ene)\n",
        "        attns = tf.nn.softmax(attn_ene, axis=1)\n",
        "\n",
        "        context_vector = attns * output\n",
        "        final_inputs = tf.reduce_sum(context_vector, axis=1)\n",
        "        final_inputs2 = tf.reduce_sum(output, axis=1)\n",
        "\n",
        "        combined_inputs = tf.concat([final_inputs, final_inputs2], axis=1)\n",
        "\n",
        "        logits = self.label(combined_inputs)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFP0wul3DVRK",
        "outputId": "31e47f0d-73e9-43d1-fa7f-0ec29f097ca3"
      },
      "source": [
        "# model hyperparameters\n",
        "INPUT_DIM = 1\n",
        "OUTPUT_DIM = 7\n",
        "HID_DIM = 128\n",
        "DROPOUT = 0.2\n",
        "RECURRENT_Layers = 1\n",
        "EPOCHS = 400\n",
        "BATCH_SIZE = 128\n",
        "num_classes = 7\n",
        "lr = 0.004\n",
        "num_gpu = 2\n",
        "SEED=15\n",
        "\n",
        "# generate dataloader\n",
        "data_path_x_train = f'{DATA_DIR}/train_x_small.csv'\n",
        "data_path_y_train = f'{DATA_DIR}/train_y_small.csv'\n",
        "data_path_x_test = f'{DATA_DIR}/test_x_small.csv'\n",
        "data_path_y_test = f'{DATA_DIR}/test_y_small.csv'\n",
        "\n",
        "# data_path_x_train = f'{DATA_DIR}/train_x_large.csv'\n",
        "# data_path_y_train = f'{DATA_DIR}/train_y_large.csv'\n",
        "# data_path_x_test = f'{DATA_DIR}/test_x_large.csv'\n",
        "# data_path_y_test = f'{DATA_DIR}/test_y_large.csv'\n",
        "\n",
        "\n",
        "df_x_train = pd.read_csv(data_path_x_train)\n",
        "df_y_train = pd.read_csv(data_path_y_train, dtype=np.int32)\n",
        "df_x_test = pd.read_csv(data_path_x_test)\n",
        "df_y_test = pd.read_csv(data_path_y_test, dtype=np.int32)\n",
        "\n",
        "\n",
        "\n",
        "df_x_train = pd.concat([df_x_train, df_x_test], axis=0)\n",
        "df_y_train = pd.concat([df_y_train, df_y_test], axis=0)\n",
        "\n",
        "\n",
        "train_X, test_X, train_y, test_y = df_x_train.to_numpy(\n",
        "), df_x_test.to_numpy(), df_y_train.to_numpy(), df_y_test.to_numpy()\n",
        "\n",
        "train_y = np.squeeze(train_y)\n",
        "test_y = np.squeeze(test_y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train_X, train_y, test_size=0.1, random_state=SEED, stratify=train_y)\n",
        "\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "\n",
        "print('X_train:{}'.format(X_train.shape))\n",
        "print('X_train:{}'.format(X_train))\n",
        "print('X_test:{}'.format(X_test.shape))\n",
        "print('y_train:{}'.format(y_train))\n",
        "print('y_train:{}'.format(y_train.shape))\n",
        "print('y_test:{}'.format(y_test.shape))\n",
        "\n",
        "\n",
        "model = AttentionModel(INPUT_DIM, HID_DIM,\n",
        "                           OUTPUT_DIM, RECURRENT_Layers, DROPOUT)\n",
        "\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                patience=10,\n",
        "                                                mode='min',restore_best_weights=True)\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f'{DATA_DIR}/small_model.h5',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer=tf.optimizers.Adam(),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "baseline_history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks = [early_stopping,model_checkpoint_callback],\n",
        "    validation_data=(X_test, y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:(21781, 41, 1)\n",
            "X_train:[[[ 0.04916661]\n",
            "  [ 0.06928869]\n",
            "  [ 0.08682726]\n",
            "  ...\n",
            "  [ 0.125618  ]\n",
            "  [ 0.11803508]\n",
            "  [ 0.1488349 ]]\n",
            "\n",
            " [[ 0.07300766]\n",
            "  [ 0.07839973]\n",
            "  [ 0.0820749 ]\n",
            "  ...\n",
            "  [ 0.2463104 ]\n",
            "  [ 0.17560796]\n",
            "  [ 0.1086869 ]]\n",
            "\n",
            " [[ 0.05702595]\n",
            "  [ 0.08256717]\n",
            "  [ 0.09899703]\n",
            "  ...\n",
            "  [ 0.09075005]\n",
            "  [ 0.08141035]\n",
            "  [-0.06551325]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.02729875]\n",
            "  [ 0.05372446]\n",
            "  [ 0.07897268]\n",
            "  ...\n",
            "  [ 0.12870689]\n",
            "  [ 0.12182612]\n",
            "  [ 0.19791627]]\n",
            "\n",
            " [[ 0.06541488]\n",
            "  [ 0.07677525]\n",
            "  [ 0.08008936]\n",
            "  ...\n",
            "  [ 0.17040706]\n",
            "  [ 0.1244928 ]\n",
            "  [ 0.08673806]]\n",
            "\n",
            " [[ 0.2587482 ]\n",
            "  [ 0.2848584 ]\n",
            "  [ 0.30300355]\n",
            "  ...\n",
            "  [ 0.24640791]\n",
            "  [ 0.24452345]\n",
            "  [ 0.22923668]]]\n",
            "X_test:(2421, 41, 1)\n",
            "y_train:[1 1 4 ... 1 1 6]\n",
            "y_train:(21781,)\n",
            "y_test:(2421,)\n",
            "Epoch 1/400\n",
            "171/171 [==============================] - 3s 18ms/step - loss: 1.0634 - accuracy: 0.5498 - val_loss: 0.8145 - val_accuracy: 0.6766\n",
            "Epoch 2/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.6574 - accuracy: 0.7047 - val_loss: 0.6060 - val_accuracy: 0.7513\n",
            "Epoch 3/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.4355 - accuracy: 0.8203 - val_loss: 0.3762 - val_accuracy: 0.8298\n",
            "Epoch 4/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.3913 - accuracy: 0.8392 - val_loss: 0.4011 - val_accuracy: 0.8368\n",
            "Epoch 5/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.3465 - accuracy: 0.8633 - val_loss: 0.2793 - val_accuracy: 0.8876\n",
            "Epoch 6/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.3014 - accuracy: 0.8903 - val_loss: 0.3825 - val_accuracy: 0.8530\n",
            "Epoch 7/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.2892 - accuracy: 0.8946 - val_loss: 0.2440 - val_accuracy: 0.9232\n",
            "Epoch 8/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.2650 - accuracy: 0.9062 - val_loss: 0.2017 - val_accuracy: 0.9372\n",
            "Epoch 9/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.2431 - accuracy: 0.9145 - val_loss: 0.2019 - val_accuracy: 0.9343\n",
            "Epoch 10/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.2315 - accuracy: 0.9216 - val_loss: 0.1865 - val_accuracy: 0.9360\n",
            "Epoch 11/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.2188 - accuracy: 0.9253 - val_loss: 0.1840 - val_accuracy: 0.9430\n",
            "Epoch 12/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.2084 - accuracy: 0.9290 - val_loss: 0.1737 - val_accuracy: 0.9430\n",
            "Epoch 13/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1966 - accuracy: 0.9346 - val_loss: 0.1794 - val_accuracy: 0.9252\n",
            "Epoch 14/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1874 - accuracy: 0.9360 - val_loss: 0.1799 - val_accuracy: 0.9314\n",
            "Epoch 15/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1890 - accuracy: 0.9341 - val_loss: 0.1353 - val_accuracy: 0.9537\n",
            "Epoch 16/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1695 - accuracy: 0.9420 - val_loss: 0.1621 - val_accuracy: 0.9434\n",
            "Epoch 17/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1738 - accuracy: 0.9397 - val_loss: 0.1362 - val_accuracy: 0.9533\n",
            "Epoch 18/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1729 - accuracy: 0.9407 - val_loss: 0.1273 - val_accuracy: 0.9533\n",
            "Epoch 19/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1554 - accuracy: 0.9479 - val_loss: 0.1606 - val_accuracy: 0.9455\n",
            "Epoch 20/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1515 - accuracy: 0.9482 - val_loss: 0.1376 - val_accuracy: 0.9517\n",
            "Epoch 21/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1545 - accuracy: 0.9480 - val_loss: 0.1478 - val_accuracy: 0.9467\n",
            "Epoch 22/400\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 0.1440 - accuracy: 0.9513 - val_loss: 0.1187 - val_accuracy: 0.9612\n",
            "Epoch 23/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1396 - accuracy: 0.9521 - val_loss: 0.1298 - val_accuracy: 0.9566\n",
            "Epoch 24/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1370 - accuracy: 0.9526 - val_loss: 0.1214 - val_accuracy: 0.9587\n",
            "Epoch 25/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1345 - accuracy: 0.9532 - val_loss: 0.1075 - val_accuracy: 0.9632\n",
            "Epoch 26/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1365 - accuracy: 0.9535 - val_loss: 0.0930 - val_accuracy: 0.9698\n",
            "Epoch 27/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1289 - accuracy: 0.9560 - val_loss: 0.1088 - val_accuracy: 0.9608\n",
            "Epoch 28/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1264 - accuracy: 0.9571 - val_loss: 0.1087 - val_accuracy: 0.9641\n",
            "Epoch 29/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1281 - accuracy: 0.9568 - val_loss: 0.1012 - val_accuracy: 0.9653\n",
            "Epoch 30/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1213 - accuracy: 0.9581 - val_loss: 0.1019 - val_accuracy: 0.9608\n",
            "Epoch 31/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1215 - accuracy: 0.9585 - val_loss: 0.0973 - val_accuracy: 0.9657\n",
            "Epoch 32/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1209 - accuracy: 0.9585 - val_loss: 0.1140 - val_accuracy: 0.9608\n",
            "Epoch 33/400\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 0.1198 - accuracy: 0.9596 - val_loss: 0.0912 - val_accuracy: 0.9723\n",
            "Epoch 34/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1101 - accuracy: 0.9633 - val_loss: 0.0896 - val_accuracy: 0.9686\n",
            "Epoch 35/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1171 - accuracy: 0.9606 - val_loss: 0.1083 - val_accuracy: 0.9620\n",
            "Epoch 36/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1180 - accuracy: 0.9601 - val_loss: 0.1153 - val_accuracy: 0.9620\n",
            "Epoch 37/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1077 - accuracy: 0.9630 - val_loss: 0.0890 - val_accuracy: 0.9674\n",
            "Epoch 38/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1138 - accuracy: 0.9618 - val_loss: 0.0903 - val_accuracy: 0.9703\n",
            "Epoch 39/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1017 - accuracy: 0.9648 - val_loss: 0.0906 - val_accuracy: 0.9682\n",
            "Epoch 40/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1064 - accuracy: 0.9635 - val_loss: 0.0827 - val_accuracy: 0.9744\n",
            "Epoch 41/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.1052 - accuracy: 0.9641 - val_loss: 0.0971 - val_accuracy: 0.9661\n",
            "Epoch 42/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.1025 - accuracy: 0.9649 - val_loss: 0.0865 - val_accuracy: 0.9703\n",
            "Epoch 43/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0990 - accuracy: 0.9667 - val_loss: 0.0782 - val_accuracy: 0.9736\n",
            "Epoch 44/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0996 - accuracy: 0.9660 - val_loss: 0.0764 - val_accuracy: 0.9723\n",
            "Epoch 45/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0947 - accuracy: 0.9675 - val_loss: 0.0878 - val_accuracy: 0.9711\n",
            "Epoch 46/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0979 - accuracy: 0.9657 - val_loss: 0.0785 - val_accuracy: 0.9723\n",
            "Epoch 47/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0909 - accuracy: 0.9689 - val_loss: 0.0779 - val_accuracy: 0.9756\n",
            "Epoch 48/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0953 - accuracy: 0.9663 - val_loss: 0.0877 - val_accuracy: 0.9665\n",
            "Epoch 49/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0889 - accuracy: 0.9699 - val_loss: 0.0873 - val_accuracy: 0.9686\n",
            "Epoch 50/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0954 - accuracy: 0.9666 - val_loss: 0.0920 - val_accuracy: 0.9665\n",
            "Epoch 51/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0955 - accuracy: 0.9674 - val_loss: 0.0834 - val_accuracy: 0.9719\n",
            "Epoch 52/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0892 - accuracy: 0.9697 - val_loss: 0.0935 - val_accuracy: 0.9694\n",
            "Epoch 53/400\n",
            "171/171 [==============================] - 2s 13ms/step - loss: 0.0906 - accuracy: 0.9678 - val_loss: 0.0711 - val_accuracy: 0.9769\n",
            "Epoch 54/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0846 - accuracy: 0.9696 - val_loss: 0.0848 - val_accuracy: 0.9756\n",
            "Epoch 55/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0903 - accuracy: 0.9687 - val_loss: 0.0905 - val_accuracy: 0.9686\n",
            "Epoch 56/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0860 - accuracy: 0.9695 - val_loss: 0.0867 - val_accuracy: 0.9694\n",
            "Epoch 57/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0911 - accuracy: 0.9692 - val_loss: 0.0861 - val_accuracy: 0.9678\n",
            "Epoch 58/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0870 - accuracy: 0.9698 - val_loss: 0.0897 - val_accuracy: 0.9703\n",
            "Epoch 59/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0891 - accuracy: 0.9690 - val_loss: 0.0843 - val_accuracy: 0.9711\n",
            "Epoch 60/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0864 - accuracy: 0.9699 - val_loss: 0.0796 - val_accuracy: 0.9719\n",
            "Epoch 61/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0802 - accuracy: 0.9722 - val_loss: 0.1199 - val_accuracy: 0.9603\n",
            "Epoch 62/400\n",
            "171/171 [==============================] - 2s 12ms/step - loss: 0.0889 - accuracy: 0.9692 - val_loss: 0.0700 - val_accuracy: 0.9752\n",
            "Epoch 63/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0786 - accuracy: 0.9735 - val_loss: 0.0784 - val_accuracy: 0.9703\n",
            "Epoch 64/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0848 - accuracy: 0.9713 - val_loss: 0.0888 - val_accuracy: 0.9711\n",
            "Epoch 65/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0863 - accuracy: 0.9697 - val_loss: 0.0795 - val_accuracy: 0.9727\n",
            "Epoch 66/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0811 - accuracy: 0.9716 - val_loss: 0.0786 - val_accuracy: 0.9711\n",
            "Epoch 67/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0912 - accuracy: 0.9679 - val_loss: 0.0898 - val_accuracy: 0.9686\n",
            "Epoch 68/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0750 - accuracy: 0.9738 - val_loss: 0.0782 - val_accuracy: 0.9736\n",
            "Epoch 69/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0755 - accuracy: 0.9731 - val_loss: 0.0835 - val_accuracy: 0.9748\n",
            "Epoch 70/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0763 - accuracy: 0.9728 - val_loss: 0.0880 - val_accuracy: 0.9723\n",
            "Epoch 71/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0844 - accuracy: 0.9704 - val_loss: 0.0860 - val_accuracy: 0.9678\n",
            "Epoch 72/400\n",
            "171/171 [==============================] - 2s 13ms/step - loss: 0.0719 - accuracy: 0.9743 - val_loss: 0.0700 - val_accuracy: 0.9752\n",
            "Epoch 73/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0748 - accuracy: 0.9741 - val_loss: 0.0734 - val_accuracy: 0.9732\n",
            "Epoch 74/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0761 - accuracy: 0.9734 - val_loss: 0.0597 - val_accuracy: 0.9810\n",
            "Epoch 75/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0714 - accuracy: 0.9747 - val_loss: 0.0779 - val_accuracy: 0.9744\n",
            "Epoch 76/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0748 - accuracy: 0.9732 - val_loss: 0.0678 - val_accuracy: 0.9748\n",
            "Epoch 77/400\n",
            "171/171 [==============================] - 2s 10ms/step - loss: 0.0777 - accuracy: 0.9735 - val_loss: 0.0758 - val_accuracy: 0.9752\n",
            "Epoch 78/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0744 - accuracy: 0.9740 - val_loss: 0.0735 - val_accuracy: 0.9732\n",
            "Epoch 79/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0747 - accuracy: 0.9728 - val_loss: 0.0679 - val_accuracy: 0.9793\n",
            "Epoch 80/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0690 - accuracy: 0.9745 - val_loss: 0.0645 - val_accuracy: 0.9760\n",
            "Epoch 81/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0745 - accuracy: 0.9737 - val_loss: 0.0788 - val_accuracy: 0.9744\n",
            "Epoch 82/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0769 - accuracy: 0.9738 - val_loss: 0.0740 - val_accuracy: 0.9748\n",
            "Epoch 83/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0771 - accuracy: 0.9718 - val_loss: 0.0689 - val_accuracy: 0.9748\n",
            "Epoch 84/400\n",
            "171/171 [==============================] - 2s 11ms/step - loss: 0.0699 - accuracy: 0.9761 - val_loss: 0.0798 - val_accuracy: 0.9740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKzPdIkpCZ8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a45075dc-7ae4-42a1-9855-2f1f1d0310bc"
      },
      "source": [
        "# Filtered data\n",
        "data_path = '/content/drive/My Drive/DLDATA/Crop/cleaned_data_25753.csv'\n",
        "df_all = pd.read_csv(data_path)\n",
        "\n",
        "# pick up only NDVI,and paddocktyp\n",
        "labels = df_all.iloc[:,2]\n",
        "df_all = df_all.iloc[:, 12:].copy()\n",
        "mask = (df_all <= 1).all(axis=1)\n",
        "df_all = df_all[mask]\n",
        "labels = labels[mask]\n",
        "X = df_all\n",
        "y = labels\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "print(le.classes_)\n",
        "class_names = le.classes_\n",
        "y = le.transform(y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=15, stratify=y)\n",
        "\n",
        "\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "\n",
        "print('X_train:{}'.format(X_train.shape))\n",
        "print('X_train:{}'.format(X_train))\n",
        "print('X_test:{}'.format(X_test.shape))\n",
        "print('y_train:{}'.format(y_train))\n",
        "print('y_train:{}'.format(y_train.shape))\n",
        "print('y_test:{}'.format(y_test.shape))\n",
        "\n",
        "# balance data\n",
        "# ros = RandomOverSampler(random_state=SEED)\n",
        "# X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "X_train_resampled, y_train_resampled = X_train, y_train\n",
        "\n",
        "# check sample no.\n",
        "# unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "# print(\"Frequency of unique values of the said array:\")\n",
        "# print(np.asarray((unique_elements, counts_elements)))\n",
        "\n",
        "\n",
        "\n",
        "# DataLoader definition\n",
        "# model hyperparameters\n",
        "INPUT_DIM = 1\n",
        "OUTPUT_DIM = 7\n",
        "HID_DIM = 128\n",
        "DROPOUT = 0.2\n",
        "RECURRENT_Layers = 1\n",
        "# LR = 0.001  # learning rate\n",
        "EPOCHS = 400\n",
        "BATCH_SIZE = 256\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "\n",
        "# unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "# weights = [1/i for i in counts_elements]\n",
        "# weights[2] = weights[2]/15\n",
        "# print(np.asarray((unique_elements, counts_elements)))\n",
        "# print(weights)\n",
        "# samples_weight = np.array([weights[t] for t in y_train])\n",
        "# samples_weights = torch.FloatTensor(samples_weight).to(device)\n",
        "# class_weights = torch.FloatTensor(weights).to(device)\n",
        "# sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, len(X_train_resampled),replacement=True)\n",
        "\n",
        "# prepare PyTorch Datasets\n",
        "\n",
        "\n",
        "model = AttentionModel(INPUT_DIM, HID_DIM,\n",
        "                           OUTPUT_DIM, RECURRENT_Layers, DROPOUT)\n",
        "\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                patience=10,\n",
        "                                                mode='min')\n",
        "\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer=tf.optimizers.Adam(),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n",
        "baseline_history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Barley' 'Canola' 'Chick Pea' 'Lentils' 'Wheat']\n",
            "X_train:(22851, 276, 1)\n",
            "X_train:[[[0.19980183]\n",
            "  [0.19323996]\n",
            "  [0.1869162 ]\n",
            "  ...\n",
            "  [0.1343711 ]\n",
            "  [0.13878852]\n",
            "  [0.14370161]]\n",
            "\n",
            " [[0.17000577]\n",
            "  [0.1663594 ]\n",
            "  [0.16286805]\n",
            "  ...\n",
            "  [0.15502408]\n",
            "  [0.16259387]\n",
            "  [0.17065004]]\n",
            "\n",
            " [[0.3475954 ]\n",
            "  [0.34541732]\n",
            "  [0.3431764 ]\n",
            "  ...\n",
            "  [0.12736091]\n",
            "  [0.1284678 ]\n",
            "  [0.12981576]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.1295554 ]\n",
            "  [0.1318962 ]\n",
            "  [0.1340546 ]\n",
            "  ...\n",
            "  [0.14080852]\n",
            "  [0.1423608 ]\n",
            "  [0.14403331]]\n",
            "\n",
            " [[0.22148573]\n",
            "  [0.21736622]\n",
            "  [0.21320131]\n",
            "  ...\n",
            "  [0.15086061]\n",
            "  [0.15091068]\n",
            "  [0.15094978]]\n",
            "\n",
            " [[0.1399711 ]\n",
            "  [0.14132032]\n",
            "  [0.14262459]\n",
            "  ...\n",
            "  [0.15678623]\n",
            "  [0.15832445]\n",
            "  [0.16002485]]]\n",
            "X_test:(2540, 276, 1)\n",
            "y_train:[4 4 4 ... 0 2 0]\n",
            "y_train:(22851,)\n",
            "y_test:(2540,)\n",
            "Epoch 1/400\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'attention_model_2/bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'attention_model_2/bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0'] when minimizing the loss.\n",
            "715/715 [==============================] - 17s 24ms/step - loss: 1.4393 - accuracy: 0.3832 - val_loss: 1.3231 - val_accuracy: 0.4547\n",
            "Epoch 2/400\n",
            " 78/715 [==>...........................] - ETA: 13s - loss: 1.2539 - accuracy: 0.4119"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-35d9d489c801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# print(model.summary())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "# checkpoint_dir = './training_checkpoints'\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "#                                  encoder=encoder,\n",
        "#                                  decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}