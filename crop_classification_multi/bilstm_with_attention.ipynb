{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of nmt_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b",
        "outputId": "be0e0855-d7f6-4935-d544-2abacee586dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = '/content/drive/My Drive/DLDATA/Crop/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEhZhSeCw21"
      },
      "source": [
        "class AttentionModel(tf.keras.Model):\n",
        "    def __init__(self, batch_size, input_dim, hidden_dim, output_dim, recurrent_layers, dropout_p):\n",
        "        super(AttentionModel, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.recurrent_layers = recurrent_layers\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.input_embeded = tf.keras.layers.Dense(\n",
        "            hidden_dim//2, activation='tanh')\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_p)\n",
        "\n",
        "        self.rnn_layers = []\n",
        "        for _ in range(0, recurrent_layers):\n",
        "\n",
        "            rnn_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim,\n",
        "                                                                           return_sequences=True,\n",
        "                                                                           return_state=True))\n",
        "            self.rnn_layers.append(rnn_layer)\n",
        "\n",
        "        self.self_attention = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(hidden_dim*2, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        self.scale = 1.0/np.sqrt(hidden_dim)\n",
        "\n",
        "        self.output_linear = tf.keras.layers.Dense(self.hidden_dim)\n",
        "        self.label = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, input_sentences, batch_size=None):\n",
        "\n",
        "        input = self.input_embeded(input_sentences)\n",
        "        input = self.dropout(input)\n",
        "\n",
        "        for i, _ in enumerate(self.rnn_layers):\n",
        "            output, forward_h, forward_c, backward_h, backward_c = self.rnn_layers[i](\n",
        "                input)\n",
        "\n",
        "        attn_ene = self.self_attention(output)\n",
        "\n",
        "        # scale\n",
        "        attn_ene = tf.math.scalar_mul(self.scale, attn_ene)\n",
        "        attns = tf.nn.softmax(attn_ene, axis=1)\n",
        "\n",
        "        context_vector = attns * output\n",
        "        final_inputs = tf.reduce_sum(context_vector, axis=1)\n",
        "        final_inputs2 = tf.reduce_sum(output, axis=1)\n",
        "\n",
        "        combined_inputs = tf.concat([final_inputs, final_inputs2], axis=1)\n",
        "\n",
        "        logits = self.label(combined_inputs)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKzPdIkpCZ8c",
        "outputId": "aaa9b733-46ff-4ea0-d988-7da819194687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Filtered data\n",
        "data_path = '/content/drive/My Drive/DLDATA/Crop/cleaned_data_25753.csv'\n",
        "df_all = pd.read_csv(data_path)\n",
        "\n",
        "# pick up only NDVI,and paddocktyp\n",
        "labels = df_all.iloc[:,2]\n",
        "df_all = df_all.iloc[:, 12:].copy()\n",
        "mask = (df_all <= 1).all(axis=1)\n",
        "df_all = df_all[mask]\n",
        "labels = labels[mask]\n",
        "X = df_all\n",
        "y = labels\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "print(le.classes_)\n",
        "class_names = le.classes_\n",
        "y = le.transform(y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=15, stratify=y)\n",
        "\n",
        "\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "\n",
        "print('X_train:{}'.format(X_train.shape))\n",
        "print('X_test:{}'.format(X_test.shape))\n",
        "print('y_train:{}'.format(y_train.shape))\n",
        "print('y_test:{}'.format(y_test.shape))\n",
        "\n",
        "# balance data\n",
        "# ros = RandomOverSampler(random_state=SEED)\n",
        "# X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "X_train_resampled, y_train_resampled = X_train, y_train\n",
        "\n",
        "# check sample no.\n",
        "# unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "# print(\"Frequency of unique values of the said array:\")\n",
        "# print(np.asarray((unique_elements, counts_elements)))\n",
        "\n",
        "\n",
        "\n",
        "# DataLoader definition\n",
        "# model hyperparameters\n",
        "INPUT_DIM = 1\n",
        "OUTPUT_DIM = 5\n",
        "HID_DIM = 64\n",
        "DROPOUT = 0.1\n",
        "RECURRENT_Layers = 2\n",
        "# LR = 0.001  # learning rate\n",
        "EPOCHS = 400\n",
        "BATCH_SIZE = 32\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "\n",
        "# unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "# weights = [1/i for i in counts_elements]\n",
        "# weights[2] = weights[2]/15\n",
        "# print(np.asarray((unique_elements, counts_elements)))\n",
        "# print(weights)\n",
        "# samples_weight = np.array([weights[t] for t in y_train])\n",
        "# samples_weights = torch.FloatTensor(samples_weight).to(device)\n",
        "# class_weights = torch.FloatTensor(weights).to(device)\n",
        "# sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, len(X_train_resampled),replacement=True)\n",
        "\n",
        "# prepare PyTorch Datasets\n",
        "\n",
        "\n",
        "model = AttentionModel(BATCH_SIZE, INPUT_DIM, HID_DIM,\n",
        "                           OUTPUT_DIM, RECURRENT_Layers, DROPOUT)\n",
        "\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                patience=10,\n",
        "                                                mode='min')\n",
        "\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer=tf.optimizers.Adam(),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n",
        "baseline_history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(X_test, y_test))\n",
        "\n",
        "# print(model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Barley' 'Canola' 'Chick Pea' 'Lentils' 'Wheat']\n",
            "X_train:(22851, 276, 1)\n",
            "X_test:(2540, 276, 1)\n",
            "y_train:(22851,)\n",
            "y_test:(2540,)\n",
            "Epoch 1/400\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/recurrent_kernel:0', 'attention_model_7/bidirectional_14/forward_lstm_14/lstm_cell_43/bias:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/recurrent_kernel:0', 'attention_model_7/bidirectional_14/backward_lstm_14/lstm_cell_44/bias:0'] when minimizing the loss.\n",
            "715/715 [==============================] - 19s 26ms/step - loss: 1.5076 - accuracy: 0.3912 - val_loss: 1.2168 - val_accuracy: 0.4220\n",
            "Epoch 2/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.2797 - accuracy: 0.4048 - val_loss: 1.2288 - val_accuracy: 0.4492\n",
            "Epoch 3/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.2306 - accuracy: 0.4162 - val_loss: 1.2063 - val_accuracy: 0.4075\n",
            "Epoch 4/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.2175 - accuracy: 0.4222 - val_loss: 1.1972 - val_accuracy: 0.4232\n",
            "Epoch 5/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.1986 - accuracy: 0.4253 - val_loss: 1.1960 - val_accuracy: 0.4264\n",
            "Epoch 6/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.1887 - accuracy: 0.4259 - val_loss: 1.2117 - val_accuracy: 0.4346\n",
            "Epoch 7/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.1737 - accuracy: 0.4345 - val_loss: 1.1462 - val_accuracy: 0.4516\n",
            "Epoch 8/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.1337 - accuracy: 0.4575 - val_loss: 1.0982 - val_accuracy: 0.4657\n",
            "Epoch 9/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.0901 - accuracy: 0.4819 - val_loss: 1.0663 - val_accuracy: 0.4843\n",
            "Epoch 10/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.0690 - accuracy: 0.4990 - val_loss: 1.0620 - val_accuracy: 0.5181\n",
            "Epoch 11/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 1.0316 - accuracy: 0.5244 - val_loss: 0.9583 - val_accuracy: 0.5815\n",
            "Epoch 12/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.9856 - accuracy: 0.5512 - val_loss: 0.9255 - val_accuracy: 0.6051\n",
            "Epoch 13/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.9265 - accuracy: 0.6001 - val_loss: 0.8785 - val_accuracy: 0.6276\n",
            "Epoch 14/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8934 - accuracy: 0.6180 - val_loss: 0.9926 - val_accuracy: 0.5543\n",
            "Epoch 15/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8771 - accuracy: 0.6262 - val_loss: 0.8608 - val_accuracy: 0.6248\n",
            "Epoch 16/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8641 - accuracy: 0.6372 - val_loss: 0.8348 - val_accuracy: 0.6685\n",
            "Epoch 17/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8486 - accuracy: 0.6427 - val_loss: 0.9984 - val_accuracy: 0.5827\n",
            "Epoch 18/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8377 - accuracy: 0.6499 - val_loss: 0.7982 - val_accuracy: 0.6791\n",
            "Epoch 19/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8234 - accuracy: 0.6567 - val_loss: 0.7812 - val_accuracy: 0.6925\n",
            "Epoch 20/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.8104 - accuracy: 0.6680 - val_loss: 0.7755 - val_accuracy: 0.6882\n",
            "Epoch 21/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7933 - accuracy: 0.6695 - val_loss: 0.7559 - val_accuracy: 0.6941\n",
            "Epoch 22/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7765 - accuracy: 0.6823 - val_loss: 0.7879 - val_accuracy: 0.6933\n",
            "Epoch 23/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7561 - accuracy: 0.6913 - val_loss: 0.7265 - val_accuracy: 0.7224\n",
            "Epoch 24/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7532 - accuracy: 0.6940 - val_loss: 0.7025 - val_accuracy: 0.7256\n",
            "Epoch 25/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7448 - accuracy: 0.6940 - val_loss: 0.7501 - val_accuracy: 0.7008\n",
            "Epoch 26/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7301 - accuracy: 0.7029 - val_loss: 0.7196 - val_accuracy: 0.7106\n",
            "Epoch 27/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7233 - accuracy: 0.7077 - val_loss: 0.7486 - val_accuracy: 0.7004\n",
            "Epoch 28/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7155 - accuracy: 0.7115 - val_loss: 0.7499 - val_accuracy: 0.7059\n",
            "Epoch 29/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7136 - accuracy: 0.7085 - val_loss: 0.6642 - val_accuracy: 0.7394\n",
            "Epoch 30/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.7069 - accuracy: 0.7149 - val_loss: 0.6784 - val_accuracy: 0.7323\n",
            "Epoch 31/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6849 - accuracy: 0.7215 - val_loss: 0.6678 - val_accuracy: 0.7346\n",
            "Epoch 32/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6863 - accuracy: 0.7201 - val_loss: 0.6460 - val_accuracy: 0.7457\n",
            "Epoch 33/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6835 - accuracy: 0.7221 - val_loss: 0.6402 - val_accuracy: 0.7531\n",
            "Epoch 34/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6840 - accuracy: 0.7234 - val_loss: 0.6454 - val_accuracy: 0.7437\n",
            "Epoch 35/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6642 - accuracy: 0.7337 - val_loss: 0.7459 - val_accuracy: 0.7161\n",
            "Epoch 36/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6604 - accuracy: 0.7340 - val_loss: 0.6465 - val_accuracy: 0.7362\n",
            "Epoch 37/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6587 - accuracy: 0.7348 - val_loss: 0.6358 - val_accuracy: 0.7551\n",
            "Epoch 38/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6501 - accuracy: 0.7366 - val_loss: 0.6361 - val_accuracy: 0.7496\n",
            "Epoch 39/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6445 - accuracy: 0.7385 - val_loss: 0.6377 - val_accuracy: 0.7350\n",
            "Epoch 40/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6368 - accuracy: 0.7448 - val_loss: 0.6388 - val_accuracy: 0.7398\n",
            "Epoch 41/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6506 - accuracy: 0.7400 - val_loss: 0.6001 - val_accuracy: 0.7677\n",
            "Epoch 42/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6339 - accuracy: 0.7445 - val_loss: 0.6380 - val_accuracy: 0.7543\n",
            "Epoch 43/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6303 - accuracy: 0.7457 - val_loss: 0.6246 - val_accuracy: 0.7504\n",
            "Epoch 44/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6272 - accuracy: 0.7477 - val_loss: 0.6280 - val_accuracy: 0.7425\n",
            "Epoch 45/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6158 - accuracy: 0.7520 - val_loss: 0.6171 - val_accuracy: 0.7512\n",
            "Epoch 46/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6291 - accuracy: 0.7444 - val_loss: 0.6041 - val_accuracy: 0.7598\n",
            "Epoch 47/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6133 - accuracy: 0.7541 - val_loss: 0.6083 - val_accuracy: 0.7594\n",
            "Epoch 48/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6128 - accuracy: 0.7513 - val_loss: 0.5991 - val_accuracy: 0.7606\n",
            "Epoch 49/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6042 - accuracy: 0.7578 - val_loss: 0.6089 - val_accuracy: 0.7575\n",
            "Epoch 50/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6030 - accuracy: 0.7613 - val_loss: 0.5798 - val_accuracy: 0.7768\n",
            "Epoch 51/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5895 - accuracy: 0.7645 - val_loss: 0.5529 - val_accuracy: 0.7831\n",
            "Epoch 52/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5920 - accuracy: 0.7630 - val_loss: 0.5943 - val_accuracy: 0.7654\n",
            "Epoch 53/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.6052 - accuracy: 0.7576 - val_loss: 0.6343 - val_accuracy: 0.7535\n",
            "Epoch 54/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5819 - accuracy: 0.7667 - val_loss: 0.5781 - val_accuracy: 0.7701\n",
            "Epoch 55/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5757 - accuracy: 0.7711 - val_loss: 0.5804 - val_accuracy: 0.7724\n",
            "Epoch 56/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5729 - accuracy: 0.7729 - val_loss: 0.5436 - val_accuracy: 0.7902\n",
            "Epoch 57/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5679 - accuracy: 0.7758 - val_loss: 0.5399 - val_accuracy: 0.7980\n",
            "Epoch 58/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5672 - accuracy: 0.7747 - val_loss: 0.5558 - val_accuracy: 0.7783\n",
            "Epoch 59/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5596 - accuracy: 0.7783 - val_loss: 0.5291 - val_accuracy: 0.7996\n",
            "Epoch 60/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5656 - accuracy: 0.7713 - val_loss: 0.5221 - val_accuracy: 0.7909\n",
            "Epoch 61/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5518 - accuracy: 0.7774 - val_loss: 0.5795 - val_accuracy: 0.7870\n",
            "Epoch 62/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5484 - accuracy: 0.7826 - val_loss: 0.5304 - val_accuracy: 0.8024\n",
            "Epoch 63/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5561 - accuracy: 0.7783 - val_loss: 0.5185 - val_accuracy: 0.8000\n",
            "Epoch 64/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5399 - accuracy: 0.7823 - val_loss: 0.5361 - val_accuracy: 0.7972\n",
            "Epoch 65/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5583 - accuracy: 0.7780 - val_loss: 0.5201 - val_accuracy: 0.7909\n",
            "Epoch 66/400\n",
            "715/715 [==============================] - 18s 26ms/step - loss: 0.5336 - accuracy: 0.7885 - val_loss: 0.5599 - val_accuracy: 0.7791\n",
            "Epoch 67/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5284 - accuracy: 0.7882 - val_loss: 0.5311 - val_accuracy: 0.7945\n",
            "Epoch 68/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5252 - accuracy: 0.7897 - val_loss: 0.5609 - val_accuracy: 0.7815\n",
            "Epoch 69/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5228 - accuracy: 0.7918 - val_loss: 0.5501 - val_accuracy: 0.7882\n",
            "Epoch 70/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5142 - accuracy: 0.7978 - val_loss: 0.5158 - val_accuracy: 0.8028\n",
            "Epoch 71/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5109 - accuracy: 0.7979 - val_loss: 0.5114 - val_accuracy: 0.7980\n",
            "Epoch 72/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5134 - accuracy: 0.7974 - val_loss: 0.5370 - val_accuracy: 0.7894\n",
            "Epoch 73/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5108 - accuracy: 0.8004 - val_loss: 0.4935 - val_accuracy: 0.8193\n",
            "Epoch 74/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5162 - accuracy: 0.7987 - val_loss: 0.5415 - val_accuracy: 0.7862\n",
            "Epoch 75/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5064 - accuracy: 0.8018 - val_loss: 0.5060 - val_accuracy: 0.7976\n",
            "Epoch 76/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.5058 - accuracy: 0.7979 - val_loss: 0.4987 - val_accuracy: 0.8020\n",
            "Epoch 77/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4952 - accuracy: 0.8026 - val_loss: 0.4696 - val_accuracy: 0.8201\n",
            "Epoch 78/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4965 - accuracy: 0.8025 - val_loss: 0.5097 - val_accuracy: 0.7992\n",
            "Epoch 79/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4848 - accuracy: 0.8113 - val_loss: 0.4727 - val_accuracy: 0.8260\n",
            "Epoch 80/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4840 - accuracy: 0.8106 - val_loss: 0.4858 - val_accuracy: 0.8165\n",
            "Epoch 81/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4882 - accuracy: 0.8082 - val_loss: 0.4997 - val_accuracy: 0.8122\n",
            "Epoch 82/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4929 - accuracy: 0.8056 - val_loss: 0.4771 - val_accuracy: 0.8169\n",
            "Epoch 83/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4748 - accuracy: 0.8149 - val_loss: 0.5464 - val_accuracy: 0.7937\n",
            "Epoch 84/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4756 - accuracy: 0.8120 - val_loss: 0.4787 - val_accuracy: 0.8201\n",
            "Epoch 85/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4669 - accuracy: 0.8200 - val_loss: 0.4799 - val_accuracy: 0.8252\n",
            "Epoch 86/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4796 - accuracy: 0.8121 - val_loss: 0.4901 - val_accuracy: 0.8098\n",
            "Epoch 87/400\n",
            "715/715 [==============================] - 18s 25ms/step - loss: 0.4691 - accuracy: 0.8217 - val_loss: 0.5040 - val_accuracy: 0.8106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "# checkpoint_dir = './training_checkpoints'\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "#                                  encoder=encoder,\n",
        "#                                  decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}